{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ceaaf83",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SQL Driven Machine Learning Tutorial: Customer Churn Prediction\n",
    "In this tutorial, we will walk through an example of using SQL-driven machine learning to predict customer churn. We will be using the [JupySQL](https://github.com/ploomber/jupysql) package to run SQL queries within a Jupyter Notebook and a public dataset available on Kaggle.\n",
    "\n",
    "Customer churn is a classification problem. In customer churn prediction, the goal is to classify customers into two categories: \n",
    "those who will churn (leave the company) and those who will not churn (remain with the company). \n",
    "\n",
    "This is a binary classification problem since there are only two possible outcomes (churn or not churn) for each customer.\n",
    "\n",
    "\n",
    "## Table of Content:\n",
    "1. Introduction of the problem, use case, and background story. \n",
    "2. Introduction to JupySQL and the dataset.\n",
    "3. EDA + fitting the model \n",
    "4. Generating a stakeholder report\n",
    "5. Questions and Answers\n",
    "\n",
    "## Prerequisites\n",
    "- Install and run a Jupyter Lab instance\n",
    "- Install the JupySQL package using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a82744",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Installed successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    %pip install jupysql duckdb duckdb-engine seaborn scikit-learn xgboost --upgrade -q\n",
    "    print(\"Installed successfully\")\n",
    "except Exception:\n",
    "    print(\"Retry installing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0890bcd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn_evaluation.plot import Rank1D\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a47c7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We'll download the Telco Customer Churn dataset from Kaggle (I found a Github link) and save it as `churn.csv`.\n",
    "\n",
    "## Initial setup\n",
    "First, let's load the JupySQL extension and connect to an SQLite database in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb4c96d7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%sql duckdb://"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef622a6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Import the dataset\n",
    "Now we will import the dataset into a new table called customer_churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce3284f-e9d8-46c6-bca3-cfe92473f6bb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/treselle-systems/customer_churn_analysis/master/WA_Fn-UseC_-Telco-Customer-Churn.csv\",\n",
    "    \"churn.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf990652-d789-47af-82da-cce6471b0e0e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can try querying the data and see it works, which means the engine is connected, my data is there locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9095dc9-e561-4a9a-8a03-fa4b6417b55c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*  duckdb://\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>customerID</th>\n",
       "            <th>gender</th>\n",
       "            <th>SeniorCitizen</th>\n",
       "            <th>Partner</th>\n",
       "            <th>Dependents</th>\n",
       "            <th>tenure</th>\n",
       "            <th>PhoneService</th>\n",
       "            <th>MultipleLines</th>\n",
       "            <th>InternetService</th>\n",
       "            <th>OnlineSecurity</th>\n",
       "            <th>OnlineBackup</th>\n",
       "            <th>DeviceProtection</th>\n",
       "            <th>TechSupport</th>\n",
       "            <th>StreamingTV</th>\n",
       "            <th>StreamingMovies</th>\n",
       "            <th>Contract</th>\n",
       "            <th>PaperlessBilling</th>\n",
       "            <th>PaymentMethod</th>\n",
       "            <th>MonthlyCharges</th>\n",
       "            <th>TotalCharges</th>\n",
       "            <th>Churn</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>7590-VHVEG</td>\n",
       "            <td>Female</td>\n",
       "            <td>0</td>\n",
       "            <td>Yes</td>\n",
       "            <td>No</td>\n",
       "            <td>1</td>\n",
       "            <td>No</td>\n",
       "            <td>No phone service</td>\n",
       "            <td>DSL</td>\n",
       "            <td>No</td>\n",
       "            <td>Yes</td>\n",
       "            <td>No</td>\n",
       "            <td>No</td>\n",
       "            <td>No</td>\n",
       "            <td>No</td>\n",
       "            <td>Month-to-month</td>\n",
       "            <td>Yes</td>\n",
       "            <td>Electronic check</td>\n",
       "            <td>29.85</td>\n",
       "            <td>29.85</td>\n",
       "            <td>No</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>5575-GNVDE</td>\n",
       "            <td>Male</td>\n",
       "            <td>0</td>\n",
       "            <td>No</td>\n",
       "            <td>No</td>\n",
       "            <td>34</td>\n",
       "            <td>Yes</td>\n",
       "            <td>No</td>\n",
       "            <td>DSL</td>\n",
       "            <td>Yes</td>\n",
       "            <td>No</td>\n",
       "            <td>Yes</td>\n",
       "            <td>No</td>\n",
       "            <td>No</td>\n",
       "            <td>No</td>\n",
       "            <td>One year</td>\n",
       "            <td>No</td>\n",
       "            <td>Mailed check</td>\n",
       "            <td>56.95</td>\n",
       "            <td>1889.5</td>\n",
       "            <td>No</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3668-QPYBK</td>\n",
       "            <td>Male</td>\n",
       "            <td>0</td>\n",
       "            <td>No</td>\n",
       "            <td>No</td>\n",
       "            <td>2</td>\n",
       "            <td>Yes</td>\n",
       "            <td>No</td>\n",
       "            <td>DSL</td>\n",
       "            <td>Yes</td>\n",
       "            <td>Yes</td>\n",
       "            <td>No</td>\n",
       "            <td>No</td>\n",
       "            <td>No</td>\n",
       "            <td>No</td>\n",
       "            <td>Month-to-month</td>\n",
       "            <td>Yes</td>\n",
       "            <td>Mailed check</td>\n",
       "            <td>53.85</td>\n",
       "            <td>108.15</td>\n",
       "            <td>Yes</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "[('7590-VHVEG', 'Female', 0, 'Yes', 'No', 1, 'No', 'No phone service', 'DSL', 'No', 'Yes', 'No', 'No', 'No', 'No', 'Month-to-month', 'Yes', 'Electronic check', 29.85, '29.85', 'No'),\n",
       " ('5575-GNVDE', 'Male', 0, 'No', 'No', 34, 'Yes', 'No', 'DSL', 'Yes', 'No', 'Yes', 'No', 'No', 'No', 'One year', 'No', 'Mailed check', 56.95, '1889.5', 'No'),\n",
       " ('3668-QPYBK', 'Male', 0, 'No', 'No', 2, 'Yes', 'No', 'DSL', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'Month-to-month', 'Yes', 'Mailed check', 53.85, '108.15', 'Yes')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT * FROM churn.csv LIMIT 3;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98beb754-63b5-4932-8704-c43294bba980",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before diving into the machine learning process, let's explore the dataset to get a better understanding of the features and target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571b3355-a149-4fe4-ac68-4373c091b509",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Total number of rows in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b017fd3-f6bd-463e-828c-82e14819bb9e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*  duckdb://\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>num_rows</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>7043</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "[(7043,)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT COUNT(*) AS num_rows FROM churn.csv;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04fa951-7c09-4003-8733-33cf5d548973",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Total number of columns in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8317ca",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before diving into the machine learning process, let's explore the dataset to get a better understanding of the features and target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM churn.csv LIMIT 3;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Connecting to multiple DBs at once\n",
    "We can easily configure multiple databases and query each for the specific data we need. \n",
    "\n",
    "For instance we can start a sqlite DB and query it directly, we can name it and we can have completely different data in it. (This can be a DWH where we need enriched data for instance):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy.engine import create_engine\n",
    "\n",
    "# Setting a sqlalchemy engine\n",
    "engine = create_engine(\"sqlite://\")\n",
    "\n",
    "# Creating a demo dataset\n",
    "df = pd.DataFrame({\"x\": range(5)})\n",
    "df.to_sql(\"numbers\", engine)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note it identifies and list the available connections we currently have:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%sql engine"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM numbers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now let's switch back to our main example with the duckdb connection:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%sql duckdb://"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can query the DB to check we can see our data:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM churn.csv LIMIT 3;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "Before diving into data preprocessing and model training, let's perform some exploratory data analysis (EDA) to gain insights into the dataset and identify potential relationships between features and the target variable (customer churn).\n",
    "\n",
    "Summary statistics\n",
    "Let's start by calculating summary statistics for the numerical columns in the dataset.\n",
    "\n",
    "## Dataset information\n",
    "#### 1. Demographic Information\n",
    "\n",
    "- `gender`: Whether the client is a female or a male (Female, Male).\n",
    "- `SeniorCitizen`: Whether the client is a senior citizen or not ( 0, 1).\n",
    "- `Partner`: Whether the client has a partner or not (Yes, No).\n",
    "- `Dependents`: Whether the client has dependents or not (Yes, No).\n",
    "\n",
    "#### 2. Customer Account Information\n",
    "\n",
    "- `tenure`: Number of months the customer has stayed with the company (Multiple different numeric values).\n",
    "- `Contract`: Indicates the customer’s current contract type (Month-to-Month, One year, Two year).\n",
    "- `PaperlessBilling`: Whether the client has paperless billing or not (Yes, No).\n",
    "- `PaymentMethod`: The customer’s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit Card (automatic)).\n",
    "- `MontlyCharges`: The amount charged to the customer monthly (Multiple different numeric values).\n",
    "- `TotalCharges`: The total amount charged to the customer (Multiple different numeric values).\n",
    "\n",
    "#### 3. Services Information\n",
    "\n",
    "- `PhoneService`: Whether the client has a phone service or not (Yes, No).\n",
    "- `MultipleLines`: Whether the client has multiple lines or not (No phone service, No, Yes).\n",
    "- `InternetServices`: Whether the client is subscribed to Internet service with the company (DSL, Fiber optic, No)\n",
    "- `OnlineSecurity`: Whether the client has online security or not (No internet service, No, Yes).\n",
    "- `OnlineBackup`: Whether the client has online backup or not (No internet service, No, Yes).\n",
    "- `DeviceProtection`: Whether the client has device protection or not (No internet service, No, Yes).\n",
    "- `TechSupport`: Whether the client has tech support or not (No internet service, No, Yes).\n",
    "- `StreamingTV`: Whether the client has streaming TV or not (No internet service, No, Yes).\n",
    "- `StreamingMovies`: Whether the client has streaming movies or not (No internet service, No, Yes)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "    AVG(tenure) AS avg_tenure,\n",
    "    AVG(MonthlyCharges) AS avg_MonthlyCharges,\n",
    "    AVG(TotalCharges) AS avg_TotalCharges\n",
    "FROM\n",
    "    churn.csv;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### First problem!\n",
    "It appears that the `TotalCharges` column is being treated as a `VARCHAR` instead of a `numeric data type`. To address this issue, let's cast the `TotalCharges` column to a numeric data type before querying and calculating the average.\n",
    "\n",
    "First, let's find out if there are any non-numeric values in the TotalCharges column:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "    TotalCharges\n",
    "FROM\n",
    "    churn.csv\n",
    "WHERE\n",
    "    NOT regexp_matches(TotalCharges, '^([0-9]+(\\.[0-9]+)?)$')\n",
    "LIMIT 10;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If there are any non-numeric values, you can either remove those rows or replace them with appropriate values. For this example, let's replace them with `NULL`. In our case we'll just cast it directly and continue with our analysis."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can cast directly the `TotalCharges` column to a numeric data type and calculate the averages. DuckDB has a built in feature for this called `TRY_CAST()`. The query from our previous cell (above, `cell 5`) should now return the average values for `tenure`, `MonthlyCharges`, and `TotalCharges`.\n",
    "\n",
    "We'll perform the exact same query as before but with the `TRY_CAST()`:\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE cleaned_churn AS\n",
    "SELECT\n",
    "    AVG(tenure) AS avg_tenure,\n",
    "    AVG(MonthlyCharges) AS avg_MonthlyCharges,\n",
    "    AVG(TRY_CAST(TotalCharges AS FLOAT)) AS avg_TotalCharges\n",
    "FROM\n",
    "    churn.csv;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%sql select * from cleaned_churn limit 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Churn distribution\n",
    "Now let's check the distribution of churn in the dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "    Churn,\n",
    "    COUNT(*) AS count,\n",
    "    COUNT(*) * 1.0 / (SELECT COUNT(*) FROM churn.csv) AS percentage\n",
    "FROM\n",
    "    churn.csv\n",
    "GROUP BY\n",
    "    Churn;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving our result directly into a dataframe\n",
    "JupySQLn allows you to save your queried results into a pandas dataframe and move to python when necessary.\n",
    "You can use `%sql`/`%%sql` with `<<` or the `=` sign:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%sql data <<\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    churn.csv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = %sql SELECT * FROM churn.csv\n",
    "data = data.DataFrame()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can plot our data (note the amount of **boilerplate code**) and see the total churn table we saw earlier in a visual form:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adding a plot for the specific data\n",
    "# create a figure\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# proportion of observation of each class\n",
    "prop_response = data[\"Churn\"].value_counts(normalize=True)\n",
    "\n",
    "# create a bar plot showing the percentage of churn\n",
    "prop_response.plot(kind=\"bar\", ax=ax, color=[\"springgreen\", \"salmon\"])\n",
    "\n",
    "# set title and labels\n",
    "ax.set_title(\n",
    "    \"Proportion of observations of the response variable\", fontsize=18, loc=\"left\"\n",
    ")\n",
    "ax.set_xlabel(\"churn\", fontsize=14)\n",
    "ax.set_ylabel(\"proportion of observations\", fontsize=14)\n",
    "ax.tick_params(rotation=\"auto\")\n",
    "\n",
    "# eliminate the frame from the plot\n",
    "spine_names = (\"top\", \"right\", \"bottom\", \"left\")\n",
    "for spine_name in spine_names:\n",
    "    ax.spines[spine_name].set_visible(False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Churn by contract type\n",
    "Let's explore the relationship between contract type and churn."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "    Contract,\n",
    "    Churn,\n",
    "    COUNT(*) AS count,\n",
    "    COUNT(*) * 1.0 / (SELECT COUNT(*) FROM churn.csv WHERE Contract = c.Contract) AS percentage\n",
    "FROM\n",
    "    churn.csv AS c\n",
    "GROUP BY\n",
    "    Contract,\n",
    "    Churn;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Churn by payment method\n",
    "We can also examine the relationship between payment methods and customer churn."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "    PaymentMethod,\n",
    "    Churn,\n",
    "    COUNT(*) AS count,\n",
    "    COUNT(*) * 1.0 / (SELECT COUNT(*) FROM churn.csv WHERE PaymentMethod = p.PaymentMethod) AS percentage\n",
    "FROM\n",
    "    churn.csv AS p\n",
    "GROUP BY\n",
    "    PaymentMethod,\n",
    "    Churn\n",
    "limit 3;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correlation matrix\n",
    "Finally, let's create a correlation matrix to explore relationships between the numerical features and customer churn."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%sql df <<\n",
    "SELECT tenure, MonthlyCharges, TRY_CAST(TotalCharges AS FLOAT) as TotalCharges, Churn FROM churn.csv;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_corr = df.DataFrame()\n",
    "data_corr[\"Churn\"] = data_corr[\"Churn\"].apply(lambda x: 1 if x == \"Yes\" else 0)\n",
    "correlation_matrix = data_corr.corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These EDA techniques provide valuable insights into the dataset and can help guide the feature engineering and model selection process. Based on the EDA results, you might consider focusing on specific features, transforming variables, or trying different machine learning algorithms to improve model performance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another cool thing we can do to simply get the imidiate insights pop out to us is use jupysql's built in functionality to view our data distribution:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%sqlcmd profile --table \"churn.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also save it as an HTML report if we'd like to!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting directly from SQL"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can save queries and then use them in a similar manner to CTEs. We'll get the monthly charges and the total charges amount (cleaned), make sure we don't have bad values in them (nulls) and we'll use this query to plot the data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%sql --save clean_not_nulls --no-execute\n",
    "SELECT *, TRY_CAST(TotalCharges AS FLOAT) as total_charges,\n",
    "FROM churn.csv\n",
    "WHERE MonthlyCharges IS NOT NULL \n",
    "AND total_charges IS NOT NULL"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can plot the two columns, but the monthly charges will be skewed since the values are not normalized, we can normalize or just plot the monthly charges seperately in the next cell."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%sqlplot boxplot --column MonthlyCharges total_charges --table clean_not_nulls --with clean_not_nulls"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%sqlplot boxplot --column MonthlyCharges --table clean_not_nulls --with clean_not_nulls"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we will look on the distribution of the data, this can help us detecting outliers that will eventually affect our model performance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%sqlplot histogram --column MonthlyCharges --table clean_not_nulls --with clean_not_nulls"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%sqlplot histogram --column tenure --table clean_not_nulls --with clean_not_nulls"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a boxplot of tenure for churned and not churned customers\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=\"Churn\", y=\"tenure\", data=data)\n",
    "plt.xlabel(\"Churn Status\")\n",
    "plt.ylabel(\"Tenure\")\n",
    "plt.title(\"Tenure Distribution for Churned vs. Not Churned Customers\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What we've seen so far?\n",
    "\n",
    "#### 1. SQL is a great tool for EDA\n",
    "#### 2. We can easily connect to multiple databases\n",
    "#### 3. We don't need boilerplate to plot\n",
    "\n",
    "___\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preprocessing\n",
    "To prepare the data for machine learning, we'll need to preprocess it. We will start by converting the categorical columns into numerical values using one-hot encoding. We'll also normalize the continuous features to bring them to the same scale. \n",
    "\n",
    "In the following example we're creating a mapping on certain values within the `Multiplelines` column."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE customer_churn_encoded AS\n",
    "SELECT\n",
    "    customerID,\n",
    "    gender,\n",
    "    SeniorCitizen,\n",
    "    Partner,\n",
    "    Dependents,\n",
    "    tenure,\n",
    "    PhoneService,\n",
    "    MultipleLines,\n",
    "    CASE\n",
    "        WHEN InternetService = 'DSL' THEN 1\n",
    "        ELSE 0\n",
    "    END AS InternetService_DSL,\n",
    "    CASE\n",
    "        WHEN InternetService = 'Fiber optic' THEN 1\n",
    "        ELSE 0\n",
    "    END AS InternetService_Fiber_optic,\n",
    "    CASE\n",
    "        WHEN InternetService = 'No' THEN 1\n",
    "        ELSE 0\n",
    "    END AS InternetService_No,\n",
    "    OnlineSecurity,\n",
    "    OnlineBackup,\n",
    "    DeviceProtection,\n",
    "    TechSupport,\n",
    "    StreamingTV,\n",
    "    StreamingMovies,\n",
    "    Contract,\n",
    "    PaperlessBilling,\n",
    "    PaymentMethod,\n",
    "    MonthlyCharges,\n",
    "    TotalCharges,\n",
    "    Churn\n",
    "FROM\n",
    "    churn.csv;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SQL isn't always the answer, for instance, take this cell above for data preprocessing, it could be shorter with python and probably more efficient. The column encoding that happens above is a total of 2 lines in the cell bellow. So for simplicity from this point onward we'll continue with python. Afterall you need to use the right tool for the job (there are companies that solves this gap for instance [MindsDB](https://jupysql.ploomber.io/en/latest/integrations/mindsdb.html))."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Consume our raw data (same one we just explored) show the columns\n",
    "data.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following cell we're applying categorical labels to each of the columns, for instance in gender, instead of having values of male/female, we'll encode it to all females 1, all males 0 (or the opposite), mapping the values to numerical."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Encode categorical columns\n",
    "categorical_columns = [\n",
    "    \"gender\",\n",
    "    \"SeniorCitizen\",\n",
    "    \"Partner\",\n",
    "    \"Dependents\",\n",
    "    \"PhoneService\",\n",
    "    \"MultipleLines\",\n",
    "    \"InternetService\",\n",
    "    \"OnlineSecurity\",\n",
    "    \"OnlineBackup\",\n",
    "    \"DeviceProtection\",\n",
    "    \"TechSupport\",\n",
    "    \"StreamingTV\",\n",
    "    \"StreamingMovies\",\n",
    "    \"Contract\",\n",
    "    \"PaperlessBilling\",\n",
    "    \"PaymentMethod\",\n",
    "    \"Churn\",\n",
    "]\n",
    "data_encoded = data[categorical_columns].apply(LabelEncoder().fit_transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data normalization\n",
    "Now, we will normalize the continuous features (tenure, MonthlyCharges, and TotalCharges). We'll convert all of the column values to numeric values, normalize it, and remove null values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We'll need to clean the Totalcharges column again\n",
    "# Convert TotalCharges to a numeric data type\n",
    "data[\"TotalCharges\"] = pd.to_numeric(data[\"TotalCharges\"], errors=\"coerce\")\n",
    "\n",
    "# Handle any missing values in TotalCharges after conversion\n",
    "data[\"TotalCharges\"].fillna(data[\"TotalCharges\"].mean(), inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Normalize numerical columns\n",
    "numerical_columns = [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
    "\n",
    "data_normalized = data[numerical_columns].apply(\n",
    "    lambda x: (x - x.min()) / (x.max() - x.min())\n",
    ")\n",
    "\n",
    "# Merge encoded and normalized DataFrames\n",
    "data_preprocessed = pd.concat([data_encoded, data_normalized], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train test split\n",
    "To train and test our model, we will split the data into training (80%) and testing (20%) sets. This 20% testing data will later use us to evaluate our model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X = data_preprocessed.drop(\"Churn\", axis=1)\n",
    "y = data_preprocessed[\"Churn\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the model\n",
    "We will now train a logistic regression model using the training data. \n",
    "\n",
    "The main goal is to understand if a customer will churn or not given its parameters."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the model\n",
    "With the model trained, we can now test it by using our test dataset (which we splitted earlier) and calculate the overall accuracy.\n",
    "\n",
    "Let's find out how accurate our model is by calculating as follows:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This will return the accuracy of our logistic regression model in predicting customer churn. \n",
    "You can experiment with different models and feature selection techniques to improve the model's performance. \n",
    "\n",
    "81% isn't that good (although as we'll see soon it's probably the best performing model on that dataset without manipulating the data) and it does show us there's something in the data that affects performance. \n",
    "Usually we'll see it earlier in the EDA stage (for instance the missing values in our table report)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Let's try improving our performance by using a different model\n",
    "The model we're using together with the data we put in affects our final accuracy. We can try a different model together with some data cleaning techniques to improve our final result. Here I'll demonstrate how a simple model change can change our bottom line.\n",
    "\n",
    "To improve the model's performance, we can try a combination of techniques such as hyperparameter tuning, feature selection, and ensemble methods. In this example, we'll use a Random Forest classifier with GridSearchCV for hyperparameter tuning and feature importance analysis for feature selection.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [None, 10, 20],\n",
    "    \"min_samples_split\": [2],\n",
    "    \"min_samples_leaf\": [1, 5],\n",
    "    \"bootstrap\": [True, False],\n",
    "}\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf, param_grid=param_grid, cv=3, verbose=2, n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Fit the model\n",
    "best_rf_model = RandomForestClassifier(\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    min_samples_split=best_params[\"min_samples_split\"],\n",
    "    min_samples_leaf=best_params[\"min_samples_leaf\"],\n",
    "    bootstrap=best_params[\"bootstrap\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "y_pred = best_rf_model.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate the accuracy, classification report, and confusion matrix\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that not only our performance didn't improve, they detiriorated. It's usually helpful to benchmark a few of the models and pick the best performing one. Usually, this has a lower impact on the overall performance compared to the quality of our data and the way we clean it before training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Importance\n",
    "\n",
    "Feature importance is a technique used in machine learning to determine the relative significance of input features in predicting the target variable. By identifying the most influential features, it helps in selecting the most relevant ones for model training, thus improving model performance and interpretability.\n",
    "\n",
    "We'll use another open source package here: [Sklearn-evaluation](https://github.com/ploomber/sklearn-evaluation), this will allow us to compare model performance, and visualize it. \n",
    "\n",
    "In the example below we'll try to evaluate our feature importance and how it affects our model:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"gender\",\n",
    "    \"SeniorCitizen\",\n",
    "    \"Partner\",\n",
    "    \"Dependents\",\n",
    "    \"PhoneService\",\n",
    "    \"MultipleLines\",\n",
    "    \"InternetService\",\n",
    "    \"OnlineSecurity\",\n",
    "    \"OnlineBackup\",\n",
    "    \"DeviceProtection\",\n",
    "    \"TechSupport\",\n",
    "    \"StreamingTV\",\n",
    "    \"StreamingMovies\",\n",
    "    \"Contract\",\n",
    "    \"PaperlessBilling\",\n",
    "    \"PaymentMethod\",\n",
    "    \"tenure\",\n",
    "    \"MonthlyCharges\",\n",
    "    \"TotalCharges\",\n",
    "]\n",
    "\n",
    "actual = X[features]\n",
    "rank1d = Rank1D(features=features)\n",
    "rank1d.feature_ranks(actual)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Feature selection**: By identifying the most relevant features, it allows for the reduction of \n",
    "input dimensions, leading to simpler and more efficient models. This can help reduce overfitting and improve generalization.\n",
    "\n",
    "**Model interpretability**: Understanding the importance of different features in a model can\n",
    "provide insights into the relationships between the features and the target variable. \n",
    "This makes the model more interpretable and can help in communicating the results to non-technical stakeholders.\n",
    "\n",
    "**Better model performance**: By focusing on the most important features, models can achieve\n",
    "better performance with less data and fewer parameters. This results in faster training times and potentially improved prediction accuracy.\n",
    "\n",
    "The main advantage in our example above, we'll try to fit the model with less data, if we get to similar results, we are more efficient."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll drop 3 columns according to the graph above, as those are the least important features:\n",
    "- `Dependents`\n",
    "- `PhoneService`\n",
    "- `SeniorCitizen`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X = data_preprocessed.drop(\n",
    "    columns=[\"Churn\", \"SeniorCitizen\", \"PhoneService\", \"Dependents\"], axis=1\n",
    ")\n",
    "y = data_preprocessed[\"Churn\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = LogisticRegression(solver=\"lbfgs\", max_iter=2000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We got a similar result (81% vs 81.5% accuracy) with 3 less columns, so essentially our model can be trained faster and save time, it all depends on what metric we're trying to optimize."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ROC curve\n",
    "Another way we can evaluate our model is the ROC curve.\n",
    "\n",
    "An **ROC (Receiver Operating Characteristic) curve** is a graphical representation of a binary classifier's performance\n",
    "across various classification thresholds. It plots the true positive rate (sensitivity) \n",
    "against the false positive rate (1-specificity), highlighting the trade-off between correctly \n",
    "identifying positive cases and incorrectly classifying negative cases."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the test data\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the FPR and TPR at various thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Calculate the AUC (Area Under the Curve)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=\"ROC curve (AUC = %0.2f)\" % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "#### 1. SQL is great for EDA, reporting and basic wrangling.\n",
    "#### 2. JupySQL allows you to avoid boilerplate code.\n",
    "#### 3. Not always SQL is the right tool for the job.\n",
    "\n",
    "___\n",
    "\n",
    "In this tutorial, we walked through a SQL-driven machine learning example using the [JupySQL](https://github.com/ploomber/jupysql) package to predict customer churn. \n",
    "\n",
    "We explored the dataset, preprocessed the data, split it into training and testing sets, trained a logistic regression model, tested it, and performed cross-validation to measure the model's performance.\n",
    "\n",
    "You can continue to experiment with different machine learning algorithms and feature selection techniques to improve your model's performance. Additionally, you can integrate this SQL-driven approach into your data pipeline to automate and scale your machine learning projects.\n",
    "\n",
    "Feel free to connect and stay in touch!\n",
    "\n",
    "Ido - ido@ploomber.io"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# What's next?\n",
    "#### - Reporting and dashboarding\n",
    "#### - Auto query optimization\n",
    "#### - Better autocomplete, syntax highlighting etc\n",
    "#### - Scaling and scheduling notebooks\n",
    "\n",
    "# Questions?\n",
    "\n",
    "### Sign up for 1 month of service for free!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "magic_args,-all",
   "main_language": "sql",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}